// <auto-generated/>
using System.Runtime.CompilerServices;
using fennecs.pools;
using fennecs.storage;

#pragma warning disable CS0414 // Field is assigned but its value is never used
// ReSharper disable file IdentifierTypo
// ReSharper disable InconsistentNaming

namespace fennecs;

public partial record Stream<C0>
{
    /// <include file='../_docs.xml' path='members/member[@name="T:JobR"]'/>
    [OverloadResolutionPriority(0b_00000010_00000001)]
    public void Job(Action<R<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobR<C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobR<C0>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobR<C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobER"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job(Action<EntityRef, R<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobER<C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobER<C0>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobER<C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUR"]'/>
    [OverloadResolutionPriority(0b_00000010_00000001)]
    public void Job<U>(U uniform, Action<U, R<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUR<U, C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobUR<U, C0>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUR<U, C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUR<U, C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobEUR<U, C0>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUR<U, C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobW"]'/>
    [OverloadResolutionPriority(0b_00000010_00000000)]
    public void Job(Action<RW<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobW<C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobW<C0>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobW<C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job(Action<EntityRef, RW<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEW<C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobEW<C0>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEW<C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUW"]'/>
    [OverloadResolutionPriority(0b_00000010_00000000)]
    public void Job<U>(U uniform, Action<U, RW<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUW<U, C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobUW<U, C0>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUW<U, C0>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUW<U, C0>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var s0 = join.Select;

                  var job = JobPool<JobEUW<U, C0>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUW<U, C0>>.Return(jobs);
    }


}


public partial record Stream<C0, C1>
{
    /// <include file='../_docs.xml' path='members/member[@name="T:JobRR"]'/>
    [OverloadResolutionPriority(0b_00000100_00000011)]
    public void Job(Action<R<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRR<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobRR<C0, C1>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRR<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job(Action<EntityRef, R<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERR<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobERR<C0, C1>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERR<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURR"]'/>
    [OverloadResolutionPriority(0b_00000100_00000011)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURR<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobURR<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURR<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURR<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEURR<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURR<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRW"]'/>
    [OverloadResolutionPriority(0b_00000100_00000010)]
    public void Job(Action<R<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRW<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobRW<C0, C1>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRW<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERW<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobERW<C0, C1>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERW<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURW"]'/>
    [OverloadResolutionPriority(0b_00000100_00000010)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURW<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobURW<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURW<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURW<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEURW<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURW<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWR"]'/>
    [OverloadResolutionPriority(0b_00000100_00000001)]
    public void Job(Action<RW<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWR<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobWR<C0, C1>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWR<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWR<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEWR<C0, C1>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWR<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWR"]'/>
    [OverloadResolutionPriority(0b_00000100_00000001)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWR<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobUWR<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWR<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWR<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEUWR<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWR<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWW"]'/>
    [OverloadResolutionPriority(0b_00000100_00000000)]
    public void Job(Action<RW<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWW<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobWW<C0, C1>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWW<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWW<C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEWW<C0, C1>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWW<C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWW"]'/>
    [OverloadResolutionPriority(0b_00000100_00000000)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWW<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobUWW<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWW<U, C0, C1>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWW<U, C0, C1>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1) = join.Select;

                  var job = JobPool<JobEUWW<U, C0, C1>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWW<U, C0, C1>>.Return(jobs);
    }


}


public partial record Stream<C0, C1, C2>
{
    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000111)]
    public void Job(Action<R<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobRRR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobERRR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000111)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobURRR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEURRR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000110)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobRRW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobERRW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000110)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobURRW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEURRW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000101)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobRWR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobERWR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000101)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobURWR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEURWR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000100)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobRWW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobERWW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000100)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobURWW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEURWW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000011)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobWRR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEWRR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000011)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobUWRR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEUWRR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000010)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobWRW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEWRW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000010)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobUWRW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEUWRW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000001)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobWWR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWR<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEWWR<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWR<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWR"]'/>
    [OverloadResolutionPriority(0b_00001000_00000001)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobUWWR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWR<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEUWWR<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWR<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000000)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobWWW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWW<C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEWWW<C0, C1, C2>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWW<C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWW"]'/>
    [OverloadResolutionPriority(0b_00001000_00000000)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobUWWW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWW<U, C0, C1, C2>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWW<U, C0, C1, C2>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2) = join.Select;

                  var job = JobPool<JobEUWWW<U, C0, C1, C2>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWW<U, C0, C1, C2>>.Return(jobs);
    }


}


public partial record Stream<C0, C1, C2, C3>
{
    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001111)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRRRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001111)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERRRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001111)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURRRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001111)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURRRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001110)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRRRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001110)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERRRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001110)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURRRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001110)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURRRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001101)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRRWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001101)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERRWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001101)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURRWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001101)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURRWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001100)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRRWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001100)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERRWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001100)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURRWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001100)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURRWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001011)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRWRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001011)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERWRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001011)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURWRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001011)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURWRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001010)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRWRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001010)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERWRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001010)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURWRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001010)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURWRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001001)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRWWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001001)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERWWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00001001)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURWWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001001)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURWWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001000)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobRWWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001000)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobERWWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00001000)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobURWWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001000)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEURWWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000111)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWRRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWRRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000111)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWRRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWRRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000110)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWRRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWRRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000110)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWRRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWRRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000101)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWRWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWRWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000101)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWRWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWRWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000100)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWRWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWRWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000100)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWRWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWRWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000011)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWWRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWWRR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000011)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWWRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWWRR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000010)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWWRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWWRW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000010)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWWRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWWRW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000001)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWWWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWR<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWWWR<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWR<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWR"]'/>
    [OverloadResolutionPriority(0b_00010000_00000001)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWWWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, R<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWR<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWWWR<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWR<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000000)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobWWWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWW<C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEWWWW<C0, C1, C2, C3>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWW<C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWW"]'/>
    [OverloadResolutionPriority(0b_00010000_00000000)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobUWWWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, RW<C3>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWW<U, C0, C1, C2, C3>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3) = join.Select;

                  var job = JobPool<JobEUWWWW<U, C0, C1, C2, C3>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWW<U, C0, C1, C2, C3>>.Return(jobs);
    }


}


public partial record Stream<C0, C1, C2, C3, C4>
{
    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011111)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011111)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011111)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011111)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011110)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011110)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011110)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011110)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011101)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011101)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011101)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011101)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011100)]
    public void Job(Action<R<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011100)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011100)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011100)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011011)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011011)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011011)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011011)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011010)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011010)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011010)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011010)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011001)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011001)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00011001)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00011001)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRRWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011000)]
    public void Job(Action<R<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRRWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRRWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRRWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERRWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011000)]
    public void Job(Action<EntityRef, R<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERRWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERRWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERRWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURRWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00011000)]
    public void Job<U>(U uniform, Action<U, R<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURRWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURRWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURRWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURRWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00011000)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURRWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURRWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURRWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010111)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010111)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010111)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010111)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010110)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010110)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010110)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010110)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010101)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010101)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010101)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010101)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010100)]
    public void Job(Action<R<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010100)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010100)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010100)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010011)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010011)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010011)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010011)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010010)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010010)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010010)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010010)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010001)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010001)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00010001)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00010001)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobRWWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010000)]
    public void Job(Action<R<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobRWWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobRWWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobRWWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobERWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010000)]
    public void Job(Action<EntityRef, R<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobERWWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobERWWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobERWWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobURWWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00010000)]
    public void Job<U>(U uniform, Action<U, R<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobURWWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobURWWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobURWWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEURWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00010000)]
    public void Job<U>(U uniform, Action<EntityRef, U, R<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEURWWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEURWWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsReadOnlyMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEURWWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001111)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001111)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001111)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001111)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001110)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001110)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001110)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001110)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001101)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001101)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001101)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001101)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001100)]
    public void Job(Action<RW<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001100)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001100)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001100)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001011)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001011)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001011)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001011)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001010)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001010)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001010)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001010)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001001)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001001)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00001001)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00001001)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWRWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001000)]
    public void Job(Action<RW<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWRWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWRWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWRWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWRWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001000)]
    public void Job(Action<EntityRef, RW<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWRWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWRWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWRWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWRWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00001000)]
    public void Job<U>(U uniform, Action<U, RW<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWRWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWRWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWRWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWRWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00001000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, R<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWRWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWRWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsReadOnlyMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWRWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000111)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWRRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000111)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000111)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWRRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000110)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWRRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000110)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000110)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWRRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000101)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWRWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000101)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000101)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWRWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000100)]
    public void Job(Action<RW<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWRWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWRWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWRWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWRWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000100)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWRWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000100)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, R<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWRWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWRWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsReadOnlyMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWRWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000011)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWRR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWWRR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWRR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWRR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000011)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWRR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000011)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, R<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWRR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWWRR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWRR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000010)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWRW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWWRW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWRW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWRW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000010)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWRW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000010)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, R<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWRW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWWRW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsReadOnlyMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWRW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000001)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWWR<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWWWR<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWWR<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWWR"]'/>
    [OverloadResolutionPriority(0b_00100000_00000001)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWWR"]'/>
    [OverloadResolutionPriority(0b_00000000_00000001)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, RW<C3>, R<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWWR<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWWWR<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsReadOnlyMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWWR<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobWWWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000000)]
    public void Job(Action<RW<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobWWWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobWWWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobWWWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEWWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job(Action<EntityRef, RW<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEWWWWW<C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEWWWWW<C0, C1, C2, C3, C4>>.Rent();

                  job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEWWWWW<C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobUWWWWW"]'/>
    [OverloadResolutionPriority(0b_00100000_00000000)]
    public void Job<U>(U uniform, Action<U, RW<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobUWWWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobUWWWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobUWWWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


    /// <include file='../_docs.xml' path='members/member[@name="T:JobEUWWWWW"]'/>
    [OverloadResolutionPriority(0b_00000000_00000000)]
    public void Job<U>(U uniform, Action<EntityRef, U, RW<C0>, RW<C1>, RW<C2>, RW<C3>, RW<C4>> action)
    {
      AssertNoWildcards();

      using var worldLock = World.Lock();
      var chunkSize = Math.Max(1, Count / Concurrency);

      Countdown = Countdown ?? new (0);
      Countdown.Reset();

      using var jobs = PooledList<JobEUWWWWW<U, C0, C1, C2, C3, C4>>.Rent();

      foreach (var table in Filtered)
      {
          using var join = table.CrossJoin<C0, C1, C2, C3, C4>(_streamTypes.AsSpan());
          if (join.Empty) continue;

          var count = table.Count; // storage.Length is the capacity, not the count.
          var partitions = count / chunkSize + Math.Sign(count % chunkSize);
          do
          {
              for (var chunk = 0; chunk < partitions; chunk++)
              {
                  Countdown.AddCount();

                  var start = chunk * chunkSize;
                  var length = Math.Min(chunkSize, count - start);

                  var (s0, s1, s2, s3, s4) = join.Select;

                  var job = JobPool<JobEUWWWWW<U, C0, C1, C2, C3, C4>>.Rent();

                  job.Uniform = uniform;job.Memory0 = s0.AsMemory(start, length);job.Type0 = s0.Expression;job.Memory1 = s1.AsMemory(start, length);job.Type1 = s1.Expression;job.Memory2 = s2.AsMemory(start, length);job.Type2 = s2.Expression;job.Memory3 = s3.AsMemory(start, length);job.Type3 = s3.Expression;job.Memory4 = s4.AsMemory(start, length);job.Type4 = s4.Expression;

                  job.World = table.World;
                  job.MemoryE = table.GetStorage<Identity>(default).AsReadOnlyMemory(start, length);
                  job.Action = action;
                  job.CountDown = Countdown;
                  jobs.Add(job);

                  ThreadPool.UnsafeQueueUserWorkItem(job, true);
              }
          } while (join.Iterate());
      }

      Countdown.Signal();
      Countdown.Wait();

      JobPool<JobEUWWWWW<U, C0, C1, C2, C3, C4>>.Return(jobs);
    }


}


